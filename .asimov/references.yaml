# References Database - RoyalBit Asimov
# Verified sources for ADRs and documentation
# Format: category â†’ sources with metadata

rag_vs_long_context:
  - title: "Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive Study and Hybrid Approach"
    url: https://arxiv.org/abs/2407.16833
    authors: ["Zhuowan Li", "Cheng Li", "Mingyang Zhang", "Qiaozhu Mei", "Michael Bendersky"]
    organization: Google DeepMind / University of Michigan
    date: 2024-07
    key_finding: "LC outperforms RAG by 3.6-13.1% depending on model"

  - title: "Long Context vs. RAG for LLMs: An Evaluation and Revisits"
    url: https://arxiv.org/abs/2501.01880
    authors: ["Xinze Li", "Yixin Cao", "Yubo Ma", "Aixin Sun"]
    organization: NTU Singapore
    date: 2025-01
    key_finding: "LC generally outperforms RAG in QA, RAG better for dialogue"

  - title: "Long Context RAG Performance of LLMs"
    url: https://www.databricks.com/blog/long-context-rag-performance-llms
    organization: Databricks
    date: 2024-08
    key_finding: "2000+ experiments, most models degrade after 32-64k tokens"

  - title: "Lost in the Middle: How Language Models Use Long Contexts"
    url: https://arxiv.org/abs/2307.03172
    authors: ["Nelson F. Liu", "Kevin Lin", "John Hewitt", "Ashwin Paranjape", "Michele Bevilacqua", "Fabio Petroni", "Percy Liang"]
    organization: Stanford
    date: 2024
    key_finding: "U-shaped performance curve - models struggle with middle-positioned information"

  - title: "Context Rot: How Increasing Input Tokens Impacts LLM Performance"
    url: https://research.trychroma.com/context-rot
    organization: Chroma
    date: 2025
    key_finding: "Popular LLMs effectively utilize only 10-20% of context"

  - title: "Contextual Retrieval"
    url: https://www.anthropic.com/news/contextual-retrieval
    organization: Anthropic
    date: 2024-09-19
    key_finding: "Reduces failed retrievals by 49%, with reranking by 67%"

  - title: "RAG vs. Context-Window in GPT-4: accuracy, cost, & latency"
    url: https://www.copilotkit.ai/blog/rag-vs-context-window-in-gpt-4
    organization: CopilotKit
    date: 2024
    key_finding: "RAG achieves same performance at 4% of the cost"

mcp_overhead:
  - title: "Code execution with MCP: Building more efficient agents"
    url: https://www.anthropic.com/engineering/code-execution-with-mcp
    authors: ["Adam Jones", "Conor Kelly"]
    organization: Anthropic
    date: 2025-11-04
    key_finding: "134K tokens before optimization, 98.7% reduction possible"

  - title: "Mitigating Token Bloat in MCP (SEP-1576)"
    url: https://github.com/modelcontextprotocol/modelcontextprotocol/issues/1576
    authors: ["Zeze Chang", "Jinyang Li", "Zhen Cao"]
    organization: Huawei
    date: 2025-09-30
    key_finding: "50-1000 tokens per tool definition"

  - title: "Reducing MCP token usage by 100x"
    url: https://www.speakeasy.com/blog/how-we-reduced-token-usage-by-100x-dynamic-toolsets-v2
    author: Chase Crumbaugh
    organization: Speakeasy
    date: 2025-11-17
    key_finding: "90-96% reduction with dynamic toolsets"

  - title: "MCP vs CLI: Benchmarking Tools for Coding Agents"
    url: https://mariozechner.at/posts/2025-08-15-mcp-vs-cli/
    author: Mario Zechner
    date: 2025-08-15
    key_finding: "MCP and CLI roughly equivalent for complex tasks"

  - title: "MCP Official Specification"
    url: https://modelcontextprotocol.io/specification/latest
    organization: Anthropic / Linux Foundation
    date: 2025-11-25
    key_finding: "Protocol revision 2025-11-25"

agentic_frameworks:
  - title: "Benchmarking Multi-Agent Architectures"
    url: https://blog.langchain.com/benchmarking-multi-agent-architectures/
    author: Will Fu-Hinthorn
    organization: LangChain
    date: 2025-06-11
    key_finding: "Single agent falls sharply at 2+ distractor domains"

  - title: "Benchmarking Single Agent Performance"
    url: https://blog.langchain.com/react-agent-benchmarking/
    organization: LangChain
    date: 2025-02-10
    key_finding: "Both more context and more tools degrade agent performance"

  - title: "Context Engineering for Agents"
    url: https://blog.langchain.com/context-engineering-for-agents/
    organization: LangChain
    date: 2025-07-02
    key_finding: "Four context failure modes identified"

  - title: "Benchmarking Agentic AI Frameworks in Analytics Workflows"
    url: https://research.aimultiple.com/agentic-analytics/
    authors: ["Cem Dilmegani", "Nazli Sipi"]
    organization: AIMultiple
    date: 2025
    key_finding: "CrewAI 37% tool success rate, LangGraph 100%"

  - title: "Research shows 'more agents' isn't a reliable path to better enterprise AI"
    url: https://venturebeat.com/orchestration/research-shows-more-agents-isnt-a-reliable-path-to-better-enterprise-ai
    organization: VentureBeat
    date: 2025-12-23
    key_finding: "17.2x error amplification with independent agents, max 3-4 agents effective"

  - title: "Don't Build Multi-Agents"
    url: https://cognition.ai/blog/dont-build-multi-agents
    organization: Cognition (Devin)
    date: 2025-06
    key_finding: "All agents should read/write to same context"

dynamic_orchestration:
  - title: "Building Effective AI Agents"
    url: https://www.anthropic.com/research/building-effective-agents
    authors: ["Erik Schluntz", "Barry Zhang"]
    organization: Anthropic
    date: 2024-12-19
    key_finding: "Simple, composable patterns beat complex frameworks"

  - title: "How we built our multi-agent research system"
    url: https://www.anthropic.com/engineering/multi-agent-research-system
    authors: ["Jeremy Hadfield", "Barry Zhang", "Kenneth Lien", "Florian Scholz", "Jeremy Fox", "Daniel Ford"]
    organization: Anthropic
    date: 2025-06-13
    key_finding: "90.2% improvement, token usage explains 80% of performance variance"

  - title: "Claude Code Subagents Documentation"
    url: https://code.claude.com/docs/en/sub-agents
    organization: Anthropic
    date: 2025
    key_finding: "Task tool + subagents architecture"

  - title: "Introducing 100K Context Windows"
    url: https://www.anthropic.com/news/100k-context-windows
    organization: Anthropic
    date: 2023-05
    key_finding: "Located needle in 72K tokens in 22 seconds"

  - title: "S-Agents: Self-organizing Agents in Open-ended Environments"
    url: https://arxiv.org/abs/2402.04578
    authors: ["Jiaqi Chen", "Yuxian Jiang", "Jiachen Lu", "Li Zhang"]
    date: 2024-02
    key_finding: "Tree of Agents with dynamic workflow"

  - title: "Multi-Agent Collaboration Mechanisms Survey"
    url: https://arxiv.org/html/2501.06322v1
    date: 2025-01
    key_finding: "Social behaviors autonomously emerge within agent groups"

context_utilization:
  - title: "Context Fragmentation Costs"
    url: https://arya.ai/blog/ai-context-fragmentation
    organization: Arya.ai
    date: 2025
    key_finding: "Multi-agent task costing $0.10 can cost $1.50 with context sharing"

  - title: "Context Engineering: Why Agents Fail in Production"
    url: https://inkeep.com/blog/context-engineering-why-agents-fail
    organization: Inkeep
    date: 2025
    key_finding: "Full-file context: 95% accuracy vs fragmented: 80%"

  - title: "Why Multi-Agent LLM Systems Fail"
    url: https://orq.ai/blog/why-do-multi-agent-llm-systems-fail
    organization: Orq.ai
    date: 2025
    key_finding: "Lack of shared context results in fragmented logic"

  - title: "Multi-Agent Coordination Failure Mitigation"
    url: https://galileo.ai/blog/multi-agent-coordination-failure-mitigation
    organization: Galileo
    date: 2025
    key_finding: "Architectural limitations in maintaining shared context"

gemini_extended_context:
  - title: "Gemini 1.5: Unlocking multimodal understanding across millions of tokens"
    url: https://arxiv.org/abs/2403.05530
    organization: Google
    date: 2024-03
    key_finding: ">99.7% recall up to 1M tokens, 99.2% at 10M tokens"

  - title: "The Needle in the Haystack Test"
    url: https://cloud.google.com/blog/products/ai-machine-learning/the-needle-in-the-haystack-test-and-how-gemini-pro-solves-it
    organization: Google Cloud
    date: 2024
    key_finding: "100% accuracy up to 107 hours of audio"

# LLM Harmful Behavior Countermeasures (ADR-050, ADR-051)
# Added: 2025-12-31

economic_incentive_countermeasures:
  - title: "Harnessing the Reasoning Economy: A Survey of Efficient Reasoning for Large Language Models"
    url: https://arxiv.org/abs/2503.24377
    date: 2025-03
    key_finding: "LRMs waste resources on overthinking while underthinking complex questions"

  - title: "Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs"
    url: https://arxiv.org/abs/2501.18585
    date: 2025-01
    key_finding: "225% more tokens on incorrect answers, 418% more thought-switching"

  - title: "Do Not Think That Much for 2+3! Tempering Reasoning Efficiency of LLMs"
    url: https://arxiv.org/abs/2503.16419
    date: 2025-03
    key_finding: "Survey on stopping LLM overthinking"

  - title: "How to Get Better Outputs from Your Large Language Model"
    url: https://developer.nvidia.com/blog/how-to-get-better-outputs-from-your-large-language-model/
    organization: NVIDIA
    date: 2024
    key_finding: "Chain-of-thought prompting and temperature tuning strategies"

  - title: "State of LLM Reasoning and Inference Scaling"
    url: https://magazine.sebastianraschka.com/p/state-of-llm-reasoning-and-inference-scaling
    author: Sebastian Raschka
    date: 2025
    key_finding: "s1 paper: 'Wait' tokens improve self-verification"

  - title: "LLM Evaluation Metrics: Everything You Need for LLM Evaluation"
    url: https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation
    organization: Confident AI
    date: 2025
    key_finding: "LLM-as-a-judge evaluation framework"

  - title: "OptimalThinkingBench: Benchmarking LLM Reasoning Economy"
    url: https://arxiv.org/html/2508.13141v1
    date: 2025-08
    key_finding: "Benchmark for measuring reasoning efficiency"

  - title: "OverThink: Slowdown Attacks on Reasoning LLMs"
    url: https://arxiv.org/abs/2502.02542
    date: 2025-02
    key_finding: "Adversarial attacks exploit reasoning inefficiency"

system_prompt_override_countermeasures:
  - title: "The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions"
    url: https://arxiv.org/abs/2404.13208
    organization: OpenAI
    date: 2024-04
    key_finding: "63% better resistance to attacks with hierarchy-aware training"

  - title: "LLM Prompt Injection Prevention Cheat Sheet"
    url: https://cheatsheetseries.owasp.org/cheatsheets/LLM_Prompt_Injection_Prevention_Cheat_Sheet.html
    organization: OWASP
    date: 2025
    key_finding: "Comprehensive defense strategies for prompt injection"

  - title: "Defending Against Indirect Prompt Injection Attacks With Spotlighting"
    url: https://arxiv.org/abs/2403.14720
    organization: Microsoft
    date: 2024-03
    key_finding: "Datamarking reduces attack success from >50% to <2%"

  - title: "Instruction Hierarchy in LLMs"
    url: https://ylanglabs.com/blogs/instruction-hierarchy-in-llms
    organization: Ylang Labs
    date: 2024
    key_finding: "Four-tier priority hierarchy documentation"

  - title: "Securing LLM Systems Against Prompt Injection"
    url: https://developer.nvidia.com/blog/securing-llm-systems-against-prompt-injection/
    organization: NVIDIA AI Red Team
    date: 2024
    key_finding: "Treat all LLM outputs as potentially malicious"

  - title: "Why LLMs Fail in Multi-Turn Conversations and How to Fix It"
    url: https://www.prompthub.us/blog/why-llms-fail-in-multi-turn-conversations-and-how-to-fix-it
    organization: PromptHub
    date: 2025
    key_finding: "40% performance drop in multi-turn vs single-turn"

  - title: "LLMs Get Lost In Multi-Turn Conversation"
    url: https://arxiv.org/abs/2505.06120
    date: 2025-05
    key_finding: "Models lock-in on early mistakes"

  - title: "AutoDefense: Multi-Agent LLM Defense against Jailbreak Attacks"
    url: https://arxiv.org/abs/2403.04783
    date: 2024-03
    key_finding: "Multi-agent approach to defense"

  - title: "Context Engineering in LLM-Based Agents"
    url: https://jtanruan.medium.com/context-engineering-in-llm-based-agents-d670d6b439bc
    date: 2025
    key_finding: "Dynamic state injection techniques"

sycophancy_countermeasures:
  - title: "Sycophancy in Large Language Models: Causes and Mitigations"
    url: https://arxiv.org/abs/2411.15287
    date: 2024-11
    key_finding: "Comprehensive survey of sycophancy causes and fixes"

  - title: "Towards Understanding Sycophancy in Language Models"
    url: https://www.anthropic.com/research/towards-understanding-sycophancy-in-language-models
    organization: Anthropic
    date: 2023
    key_finding: "RLHF-trained models consistently exhibit sycophancy"

  - title: "Sycophancy is the first LLM dark pattern"
    url: https://www.seangoedecke.com/ai-sycophancy/
    author: Sean Goedecke
    date: 2024
    key_finding: "Sycophancy as systematic design flaw"

  - title: "Understanding Sycophancy in LLMs"
    url: https://www.giskard.ai/knowledge/when-your-ai-agent-tells-you-what-you-want-to-hear-understanding-sycophancy-in-llms
    organization: Giskard
    date: 2024
    key_finding: "Detection and mitigation strategies"

  - title: "How to Make ChatGPT Brutally Honest"
    url: https://medium.com/@MyDigitalMusings/how-to-make-chatgpt-brutally-honest-a59584cd5cb8
    date: 2024
    key_finding: "Practical anti-sycophancy prompts"

  - title: "Stop Being So Nice: How to Avoid AI Complacency"
    url: https://jokiruiz.com/software/stop-being-so-nice-how-to-avoid-ai-complacency-in-chatgpt-and-beyond/
    author: Joki Ruiz
    date: 2024
    key_finding: "Devil's advocate and third-person techniques"

  - title: "Detecting and Evaluating Sycophancy Bias"
    url: https://huggingface.co/blog/Rakshit122/sycophantic-ai
    organization: Hugging Face
    date: 2024
    key_finding: "Benchmark methodologies for sycophancy detection"

  - title: "SycEval: Evaluating LLM Sycophancy"
    url: https://arxiv.org/abs/2502.08177
    date: 2025-02
    key_finding: "58% sycophancy rate across major models"

  - title: "ELEPHANT: Measuring Social Sycophancy in LLMs"
    url: https://arxiv.org/abs/2505.13995
    date: 2025-05
    key_finding: "Open-ended sycophancy evaluation benchmark"

  - title: "Findings from Anthropic-OpenAI Alignment Evaluation"
    url: https://alignment.anthropic.com/2025/openai-findings/
    organization: Anthropic
    date: 2025
    key_finding: "Joint evaluation confirms sycophancy in all models"

  - title: "Simple synthetic data reduces sycophancy in large language models"
    url: https://arxiv.org/abs/2308.03958
    organization: Google Research
    date: 2023-08
    key_finding: "Training-level mitigation via synthetic data"

output_verification:
  - title: "LLM Hallucinations in 2025: How to Understand and Tackle AI's Most Persistent Quirk"
    url: https://www.lakera.ai/blog/guide-to-hallucinations-in-large-language-models
    organization: Lakera
    date: 2025
    key_finding: "Comprehensive guide to hallucination mitigation"

  - title: "SafetyNet: Detecting Harmful Outputs in LLMs"
    url: https://arxiv.org/abs/2505.14300
    date: 2025-05
    key_finding: "96% accuracy in detecting harmful outputs"

  - title: "Detecting hallucinations in large language models using semantic entropy"
    url: https://www.nature.com/articles/s41586-024-07421-0
    organization: Nature
    date: 2024
    key_finding: "AUROC 0.790 for hallucination detection via semantic entropy"

  - title: "Hallucination Detection in LLMs with Metamorphic Relations (MetaQA)"
    url: https://arxiv.org/abs/2502.15844
    date: 2025-02
    key_finding: "Self-contained detection requiring no external resources"

  - title: "OpenFactCheck: Building Customized Fact-Checking Systems"
    url: https://arxiv.org/abs/2405.05583
    date: 2024-05
    key_finding: "90%+ claims factually correct, but logical failures persist"

  - title: "Prevent factual errors from LLM hallucinations with Automated Reasoning"
    url: https://aws.amazon.com/blogs/aws/prevent-factual-errors-from-llm-hallucinations-with-mathematically-sound-automated-reasoning-checks-preview/
    organization: AWS
    date: 2024-12
    key_finding: "Mathematical logic-based verification"

  - title: "10 LLM Security Tools to Know in 2025"
    url: https://www.pynt.io/learning-hub/llm-security/10-llm-security-tools-to-know
    organization: Pynt
    date: 2025
    key_finding: "Overview of LLM Guard, Garak, Rebuff, Vigil"

  - title: "LLM cross-validation frameworks: Mitigating hallucinations"
    url: https://journalwjaets.com/node/800
    organization: World Journal of Advanced Engineering
    date: 2025
    key_finding: "Dual-LLM verification reduces hallucinations by 86%"

  - title: "A scalable framework for evaluating multiple language models"
    url: https://www.nature.com/articles/s41598-025-15203-5
    organization: Scientific Reports
    date: 2025
    key_finding: "Cross-provider validation framework"

  - title: "DarkPatterns-LLM: A Multi-Layer Benchmark"
    url: https://arxiv.org/abs/2512.22470
    date: 2025-12
    key_finding: "Seven harm categories diagnostic framework"

  - title: "OWASP Top 10 for LLMs 2025"
    url: https://www.invicti.com/blog/web-security/owasp-top-10-risks-llm-security-2025
    organization: Invicti
    date: 2025
    key_finding: "Updated LLM security risks"

  - title: "LLM Guard - Open Source Security Toolkit"
    url: https://github.com/protectai/llm-guard
    organization: Protect AI
    license: MIT
    key_finding: "Prompt injection detection and output sanitization"
